{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe14bc4-b363-49c1-a210-5936fbf05b6f",
   "metadata": {},
   "source": [
    "# TV Nova project\n",
    "- fill the description\n",
    "\n",
    "### Task 1 - data analysis\n",
    "\n",
    "#### Project Description\n",
    "\n",
    "- Please download the data.\n",
    "- Analyze the data and try to draw conclusions about the channels.\n",
    "- We have selected four channels (”channel id”) for you.\n",
    "- The time column is labelled ”timeslot datetime from” and the ID column of the movies is labelled ”main indent”.\n",
    "- This task encourages the use of Python, but it is not a requirement.\n",
    "\n",
    "#### Solution\n",
    "- analyze the data \n",
    " - undertand it (check number of rows and columns),\n",
    " - data types, missing values, empty columns, etc.\n",
    "\n",
    "#### COLUMNS ####\n",
    "- **channel_id**: channels (like Nova Cinema, Nova Sport, Nova Gold)\n",
    "- **timeslot_datetime_from**: time slot for each movie\n",
    "- **main_ident**: ID for movies\n",
    "\n",
    "#### Attached files\n",
    "- **XXX.py**: This script scraping data from ...\n",
    "\n",
    "\n",
    "### Task 2 - Prediction\n",
    "\n",
    "#### Project Description\n",
    "\n",
    "- There are two targets: ”share 15 54”, which we aim to predict, and ”share 15 54 3mo mean”, which is trivial to predict.\n",
    "- Utilize the features and time column (”timeslot datetime from”) to predict ”share 15 54” without using ”share 15 54 3mo mean”.\n",
    "- For Data Scientist: you should be able to explain what influences the prediction for any model you produce and we want to improve the quality of prediction on future sample.\n",
    "\n",
    "\n",
    "## Attached files\n",
    "- **XXX.py**: This script scraping data from ...\n",
    "\n",
    "### Task 3 - Improving prediction\n",
    "\n",
    "#### Project Description\n",
    "- Leverage ”share 15 54 3mo mean” to enhance the prediction for ”share 15 54”.\n",
    "- You may create any new features and use any black-box model that can be explained to a certain extent.\n",
    "- Explain why you chose the model you did.\n",
    "- Consider the cost of running such a model, and weigh its pros and cons.\n",
    "- Use the last month for prediction, and explain on this sample where your model performs well and where it falls short.\n",
    "- Suggest future steps for improving the data, features, and method used. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71926b04-6780-4b31-82fa-a1005d1c3b5d",
   "metadata": {},
   "source": [
    "### 1. Data analysis\n",
    "\n",
    "#### COLUMNS ####\n",
    "- **channel_id**: channels (like Nova Cinema, Nova Sport, Nova Gold)\n",
    "- **timeslot_datetime_from**: provides information about when the program or movie was aired\n",
    "- **main_ident**: ID for movies\n",
    "- **chX_X__X**: connected with channel_id (e.g. from **ch3__f_1** to\t**ch3__f_12** are months for each channel)\n",
    "- **share_15_54**: This column denotes the viewer share within the demographic group aged 15-54 for the specific timeslot and program.\n",
    "- **share_15_54_3mo_mean**: This column represents the 3-month mean of viewer share within the demographic group aged 15-54. It could be used as a feature for predicting future viewer share.\n",
    "- **ch9__f_1, ch9__f_2, ch9__f_3**: These columns appear to be features related to Channel 9. The specific meanings of these features would depend on the context provided by the dataset documentation or domain knowledge. They could represent various characteristics or attributes of the channel, such as programming genre, audience demographics, or marketing strategies.\n",
    "- **ch3__f_1, ch3__f_2, ..., ch3__f_12**: Similarly, these columns represent features related to Channel 3. The suffix \"_f_1\", \"_f_2\", etc., may indicate different feature categories or types.\n",
    "- **ch54__f_10, ch54__f_11, ch54__f_12**: These columns appear to be features related to Channel 54, following a similar naming convention as the previous channels.\n",
    "- **ch4__f_1, ch4__f_2, ..., ch4__f_12**: These columns represent features related to Channel 4, again following a similar naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfed8e2-e430-4734-af7d-302b42593b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI - Nova TV Interview Assignment_03_2024.pdf\n",
      "data_1.csv\n",
      "data_2.csv\n",
      "tv_nova.ipynb\n"
     ]
    }
   ],
   "source": [
    "# showing the name of the csv files\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f953b-da95-4844-9718-5493cde3625e",
   "metadata": {},
   "source": [
    "### Prediction preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60379558-7055-49e5-92e0-621be9261902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/martin/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in /Users/martin/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.12.0)\n",
      "Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m603.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e993e7d-c952-4482-b2f4-66e8a04d0a6d",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60180420-a76e-45ff-9053-8ee9149b8834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 11:34:38,122 - INFO - Loading data from file: data_1.csv\n",
      "2024-03-22 11:34:38,352 - INFO - Data dimensions after loading: (34160, 53)\n",
      "2024-03-22 11:34:38,385 - INFO - Data dimensions after dropping duplicated rows: (17908, 53)\n",
      "2024-03-22 11:34:38,391 - INFO - Data dimensions after dropping columns with just NULL values: (17908, 48)\n",
      "2024-03-22 11:34:38,395 - INFO - Data dimensions after dropping rows containing NULL values: (2860, 48)\n",
      "2024-03-22 11:34:38,396 - INFO - Generating new features: day_of_week, month_of_year, hour_of_day and season\n",
      "/var/folders/n8/kqvg6ld10zjdvxlx1cwnk09m0000gn/T/ipykernel_84409/2441042856.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['day_of_week'] = data['timeslot_datetime_from'].dt.day_name()\n",
      "2024-03-22 11:34:38,407 - INFO - Data dimensions after encoding categorical features: (2860, 158)\n",
      "2024-03-22 11:34:38,408 - WARNING - Scaling of features was skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ch3__f_10', 'ch3__f_11', 'ch54__f_10', 'ch54__f_11', 'ch4__f_10',\n",
      "       'ch4__f_11', 'day_of_week', 'month_of_year', 'season'],\n",
      "      dtype='object')\n",
      "Dimension of training data: (2804, 158)\n",
      "Dimension of testing data: (56, 158)\n",
      "Evaluation Metrics:\n",
      "Mean Squared Error (MSE): 11.07610638868297\n",
      "Mean Absolute Error (MAE): 2.5626147447095735\n",
      "Root Mean Squared Error (RMSE): 3.3280784829512315\n",
      "R-squared (R^2) Score: 0.7777350453708118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, r2_score\n",
    "\n",
    "class SharePredictionModel:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load CSV data from the specified file path and convert the 'timeslot_datetime_from'\n",
    "        column to timestamp format.\n",
    "    \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame containing the loaded data.\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading data from file: %s\", self.file_path)\n",
    "        data = pd.read_csv(self.file_path, low_memory=False)\n",
    "\n",
    "        # Convert timeslot_datetime_from from object to timestamp format\n",
    "        data['timeslot_datetime_from'] = pd.to_datetime(data['timeslot_datetime_from'])\n",
    "\n",
    "        data = data.sort_values(by=['channel_id', 'main_ident', 'timeslot_datetime_from'])\n",
    "        logging.info(\"Data dimensions after loading: %s\", data.shape)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def process_data(self, data, duplicate_rows=False):\n",
    "        \"\"\"\n",
    "        Perform data processing on the DataFrame.\n",
    "    \n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the CSV data.\n",
    "            duplicate_rows (bool): Flag to indicate whether to duplicate rows if duplicated. \n",
    "                                    If True, duplicates rows; if False, does not duplicate rows.\n",
    "    \n",
    "        Returns:\n",
    "            DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        if duplicate_rows:\n",
    "            # Drop duplicated rows\n",
    "            data.drop_duplicates(inplace=True)\n",
    "            logging.info(\"Data dimensions after dropping duplicated rows: %s\", data.shape)\n",
    "            \n",
    "        else:\n",
    "            # Keep duplicated rows\n",
    "            logging.info(\"Data dimensions when skipping duplicating: %s\", data.shape)\n",
    "    \n",
    "        # Drop columns with just NULL values\n",
    "        data.dropna(axis=1, how='all', inplace=True)\n",
    "        logging.info(\"Data dimensions after dropping columns with just NULL values: %s\", data.shape)\n",
    "    \n",
    "        # Drop rows containing NULL values\n",
    "        data.dropna(axis=0, how='any', inplace=True)\n",
    "        logging.info(\"Data dimensions after dropping rows containing NULL values: %s\", data.shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def select_features(self, data, include_main_ident=False, include_share_15_54_3mo_mean=True):\n",
    "        \"\"\"\n",
    "        Select features to include or exclude from the DataFrame.\n",
    "    \n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the CSV data.\n",
    "            include_main_ident (bool): Flag to include or exclude 'main_ident' column. Default is True.\n",
    "            include_share_15_54_3mo_mean (bool): Flag to include or exclude 'share_15_54_3mo_mean' column. Default is True.\n",
    "    \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with selected features.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Select all features by default\n",
    "        all_features = list(data.columns)\n",
    "    \n",
    "        # Exclude features based on flags (using list comprehension)\n",
    "        if not include_main_ident:\n",
    "            all_features = [col for col in all_features if col != 'main_ident']\n",
    "        if not include_share_15_54_3mo_mean:\n",
    "            all_features = [col for col in all_features if col != 'share_15_54_3mo_mean']\n",
    "    \n",
    "        # Return the original DataFrame if no exclusions were made\n",
    "        return data if all_features == list(data.columns) else data[all_features]\n",
    "\n",
    "        \n",
    "    def extract_datetime_features(self, data, enable=False):\n",
    "        \"\"\"\n",
    "        Extract datetime features from the timeslot_datetime_from column.\n",
    "\n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the CSV data with 'timeslot_datetime_from' column.\n",
    "            enable (bool): Flag to enable/disable datetime feature extraction. Default is True.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with extracted datetime features.\n",
    "        \"\"\"\n",
    "        if not enable:\n",
    "            logging.info(\"Datetime feature extraction is disabled. Skipping...\")\n",
    "            return data\n",
    "\n",
    "        else:\n",
    "            logging.info(\"Generating new features: day_of_week, month_of_year, hour_of_day and season\")\n",
    "\n",
    "            data['day_of_week'] = data['timeslot_datetime_from'].dt.day_name()\n",
    "            data['month_of_year'] = data['timeslot_datetime_from'].dt.month_name()\n",
    "            data['hour_of_day'] = data['timeslot_datetime_from'].dt.hour\n",
    "    \n",
    "            # Define seasons based on months\n",
    "            month_to_season = {\n",
    "                1: 'Winter', 2: 'Winter', 3: 'Spring',\n",
    "                4: 'Spring', 5: 'Spring', 6: 'Summer',\n",
    "                7: 'Summer', 8: 'Summer', 9: 'Fall',\n",
    "                10: 'Fall', 11: 'Fall', 12: 'Winter'\n",
    "            }\n",
    "            data['season'] = data['timeslot_datetime_from'].dt.month.map(month_to_season)\n",
    "    \n",
    "            return data\n",
    "\n",
    "            \n",
    "    def encode_categorical_features(self, data):\n",
    "        \"\"\"\n",
    "        Encode categorical features using one-hot encoding.\n",
    "\n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the CSV data.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with categorical features encoded using one-hot encoding.\n",
    "        \"\"\"\n",
    "        # Get list of object columns (categorical features)\n",
    "        cat_columns = data.select_dtypes(include=['object', 'bool']).columns\n",
    "        print(cat_columns)\n",
    "\n",
    "        # Perform one-hot encoding\n",
    "        data_encoded = pd.get_dummies(data, columns=cat_columns, dtype=int)\n",
    "\n",
    "        logging.info(\"Data dimensions after encoding categorical features: %s\", data_encoded.shape)\n",
    "        return data_encoded\n",
    "\n",
    "    \n",
    "    def scale_data(self, data, method='minmax', applied=False):\n",
    "        \"\"\"\n",
    "        Apply scaling to the encoded data.\n",
    "    \n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the encoded data.\n",
    "            method (str): Scaling method to use: 'minmax' for Min-Max Scaling or 'standard' for Standard Scaling.\n",
    "                Default is 'minmax'.\n",
    "            applied (bool): Whether to apply scaling or not. If False, the function will return the original data without scaling.\n",
    "    \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with scaled features if applied is True, otherwise returns the original data.\n",
    "        \"\"\"\n",
    "        if not applied:\n",
    "            logging.warning(\"Scaling of features was skipped.\")\n",
    "            return data\n",
    "        \n",
    "        if method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            logging.warning(\"Invalid scaling method. Using Min-Max Scaling by default.\")\n",
    "            scaler = MinMaxScaler()\n",
    "    \n",
    "        scaled_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "        logging.info(\"Data dimensions after scaling: %s\", scaled_data.shape)\n",
    "        return scaled_data\n",
    "\n",
    "\n",
    "    def split_data(self, data):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "    \n",
    "        Parameters:\n",
    "            data (DataFrame): DataFrame containing the CSV data.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: Tuple containing train and test DataFrames.\n",
    "        \"\"\"\n",
    "        # Extract the latest month for testing\n",
    "        latest_month = data['timeslot_datetime_from'].max().month\n",
    "        latest_year = data['timeslot_datetime_from'].max().year\n",
    "        test_data = data[(data['timeslot_datetime_from'].dt.month == latest_month) & \n",
    "                         (data['timeslot_datetime_from'].dt.year == latest_year)]\n",
    "    \n",
    "        # Use the remaining data for training\n",
    "        train_data = data[data['timeslot_datetime_from'] < test_data['timeslot_datetime_from'].min()]\n",
    "        print(\"Dimension of training data:\", train_data.shape)\n",
    "        print(\"Dimension of testing data:\", test_data.shape)\n",
    "    \n",
    "        return train_data, test_data\n",
    "\n",
    "\n",
    "    def train_model(self, train_data):\n",
    "        \"\"\"\n",
    "        Train the regression model.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (DataFrame): DataFrame containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            model: Trained regression model.\n",
    "        \"\"\"\n",
    "        # Select features and target variable\n",
    "        X = train_data.drop(columns=['share_15_54', 'timeslot_datetime_from'])\n",
    "        y = train_data['share_15_54']\n",
    "\n",
    "        # Choose a regression model \n",
    "        # We could use also other models like Gradient Boosting Regressor, XGBoost regressor, etc.\n",
    "        # model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "        # model = RandomForestRegressor()\n",
    "        model = GradientBoostingRegressor()\n",
    "        # model = xgb.XGBClassifier()\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "    \n",
    "\n",
    "    \n",
    "    def evaluate_model(self, test_data, test_predictions):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of the regression model.\n",
    "\n",
    "        Parameters:\n",
    "            test_data (DataFrame): DataFrame containing the testing data.\n",
    "            test_predictions (array-like): Predicted values for the testing data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        # Append 'timeslot_datetime_from' column back to the test data\n",
    "        test_data_with_predictions = test_data.copy()\n",
    "        test_data_with_predictions['timeslot_datetime_from'] = test_data['timeslot_datetime_from']\n",
    "        \n",
    "        # Calculate Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(test_data['share_15_54'], test_predictions)\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(test_data['share_15_54'], test_predictions)\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE)\n",
    "        rmse = root_mean_squared_error(test_data['share_15_54'], test_predictions)\n",
    "\n",
    "        # Calculate R-squared (R^2) score\n",
    "        r2 = r2_score(test_data['share_15_54'], test_predictions)\n",
    "\n",
    "        # Create a dictionary to store the evaluation metrics\n",
    "        evaluation_metrics = {\n",
    "            'Mean Squared Error (MSE)': mse,\n",
    "            'Mean Absolute Error (MAE)': mae,\n",
    "            'Root Mean Squared Error (RMSE)':rmse,\n",
    "            'R-squared (R^2) Score': r2\n",
    "        }\n",
    "\n",
    "        return evaluation_metrics, test_data_with_predictions\n",
    "\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create an instance of the SharePredictionModel class with the CSV file path\n",
    "model = SharePredictionModel('data_1.csv')\n",
    "# Load CSV data and log the dimensions of the DataFrame\n",
    "data = model.load_data()\n",
    "# Process the loaded data\n",
    "processed_data = model.process_data(data, duplicate_rows=True)\n",
    "# Training the model with or without a specific features\n",
    "selected_features = model.select_features(data=processed_data, include_main_ident=False, include_share_15_54_3mo_mean=False)\n",
    "# Extract datetime features\n",
    "data_with_datetime_features = model.extract_datetime_features(data=selected_features, enable=True)\n",
    "# Encode categorical features\n",
    "data_encoded = model.encode_categorical_features(data=data_with_datetime_features)\n",
    "# Apply Scaling to the encoded data\n",
    "scaled_data_minmax = model.scale_data(data_encoded, method='minmax', applied=False)\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = model.split_data(data=data_encoded)\n",
    "# Train the regression model\n",
    "regression_model = model.train_model(train_data=train_data)\n",
    "# Make predictions on the test data\n",
    "test_predictions = regression_model.predict(test_data.drop(columns=['share_15_54', 'timeslot_datetime_from']))\n",
    "# Evaluate model performance\n",
    "evaluation_metrics, test_data_with_predictions = model.evaluate_model(test_data=test_data, test_predictions=test_predictions)\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "     print(f\"{metric}: {value}\")\n",
    "# Create a DataFrame to compare predicted and real values\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'timeslot_datetime_from': test_data_with_predictions['timeslot_datetime_from'],  # Timestamp column\n",
    "#     'real_share_15_54': test_data_with_predictions['share_15_54'],  # Real values\n",
    "#     'predicted_share_15_54': test_predictions  # Predicted values\n",
    "# })\n",
    "\n",
    "# Print the comparison DataFrame\n",
    "# print(\"Comparison of Real and Predicted Values:\")\n",
    "# print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90d965-e697-4590-8d58-b20a85da2934",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE): 11.34771151020403\n",
    "Mean Absolute Error (MAE): 2.5737596548568185\n",
    "R-squared (R^2) Score: 0.7851847551230013"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
